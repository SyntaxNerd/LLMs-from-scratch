{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYzl4QXOmnM6uzMbIouFo9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing GPT-2 architecture"
      ],
      "metadata": {
        "id": "yIcPajWvpde3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlER_AzvpXVq"
      },
      "outputs": [],
      "source": [
        "# gpt 2 config\n",
        "GPT_2_CONFIG = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"embedding_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "y6T_SQ2xrxJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-2 placeholder\n",
        "class DummyGPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"embedding_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"embedding_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "        self.tsf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "        self.final_norm = DummyLayerNorm(cfg[\"embedding_dim\"])\n",
        "        self.out_heads = nn.Linear(cfg[\"embedding_dim\"], cfg[\"vocab_size\"], bias = False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        token_embeds = self.token_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
        "        x = token_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.tsf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_heads(x)\n",
        "        return logits\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps = 1e-5):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "metadata": {
        "id": "IIFz3-XdqIwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample input\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim = 0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkPpkIcmumoS",
        "outputId": "ba60d2dd-94a9-4ee9-f49b-ff753a63eb4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instance of GPT class\n",
        "model = DummyGPTModel(GPT_2_CONFIG)\n",
        "logits = model(batch)\n",
        "print(\"Logits shape:\", logits.shape)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQVIbrGfxGjc",
        "outputId": "f657292e-2276-44b4-9e2f-6bacb5e37aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[ 0.0826, -0.2156,  1.6275,  ..., -1.2274, -0.6845, -0.8324],\n",
            "         [ 0.1974,  0.6233, -0.1014,  ...,  0.1794,  1.2948,  2.0486],\n",
            "         [ 0.2494,  1.9467,  1.1572,  ..., -0.0411,  2.2900,  0.6426],\n",
            "         [-0.3342,  0.5649,  0.5333,  ...,  0.2540,  0.3622, -0.3424]],\n",
            "\n",
            "        [[ 0.0602, -0.2113,  1.6655,  ..., -0.9637, -0.6737, -0.9095],\n",
            "         [ 1.3411,  0.9896, -0.0440,  ...,  0.0917,  1.0057, -0.2773],\n",
            "         [-0.4158,  1.5241,  0.5208,  ...,  0.3793,  1.0881, -0.5831],\n",
            "         [ 0.9586, -1.3280,  0.5330,  ...,  0.2154, -0.1688, -0.1250]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# layer normalization class\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(embedding_dim))\n",
        "        self.shift = nn.Parameter(torch.ones(embedding_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim = -1, keepdim = True)\n",
        "        variance = x.var(dim = -1, keepdim = True, unbiased = True)\n",
        "        normalized_x = (x - mean) / torch.sqrt(variance + self.eps)\n",
        "        return self.scale * normalized_x + self.shift"
      ],
      "metadata": {
        "id": "RXF_uSCE9H1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_example = torch.randn(2, 5)\n",
        "print(batch_example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh-iU2G4AObC",
        "outputId": "67d0910d-6819-43cb-9ac6-7deea32926ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.6206,  0.7020, -2.6237, -0.6853,  0.3450],\n",
            "        [ 0.5030, -0.7696,  0.8332,  2.0799,  1.1490]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ln = LayerNorm(embedding_dim = 5)\n",
        "out_ln = ln(batch_example)\n",
        "mean = out_ln.mean(dim = -1, keepdim = True)\n",
        "var = out_ln.var(dim = -1, keepdim = True, unbiased = True)\n",
        "print(mean)\n",
        "print(var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqK5a5Z4yPeE",
        "outputId": "d84cb6f5-ed06-4508-e4db-5a39148d2245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [1.]], grad_fn=<MeanBackward1>)\n",
            "tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Feed Forward NN with GELU activation function"
      ],
      "metadata": {
        "id": "Os_-3VvtBHAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GELU activation function\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "JTI4BjGXAGJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feed forward layer\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"embedding_dim\"], 4 * cfg[\"embedding_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear(4 * cfg[\"embedding_dim\"], cfg[\"embedding_dim\"]),\n",
        "    )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "d8J9iakKDNXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffn = FeedForward(GPT_2_CONFIG)\n",
        "x = torch.rand(2, 3, 768)\n",
        "out = ffn(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMioJc15D0T5",
        "outputId": "a15d772c-79e1-4f1c-cabd-19dadbe68c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shortcut connections"
      ],
      "metadata": {
        "id": "ifUPxT0YE5Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class\n",
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "    def __init__(self, layer_sizes, use_shortcuts):\n",
        "        super().__init__()\n",
        "        self.use_shortcut = use_shortcuts\n",
        "        self.layers = nn.ModuleList([\n",
        "            # Implement 5 layers\n",
        "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            layer_output = layer(x)\n",
        "            if self.use_shortcut and x.shape == layer.output.shape:\n",
        "                x += layer_output\n",
        "            else:\n",
        "                x = layer_output\n",
        "        return x"
      ],
      "metadata": {
        "id": "0uO1QlLPEcl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "        self.W_query = torch.nn.Linear(d_in, d_out, qkv_bias)\n",
        "        self.W_key = torch.nn.Linear(d_in, d_out, qkv_bias)\n",
        "        self.W_value = torch.nn.Linear(d_in, d_out, qkv_bias)\n",
        "        self.out_proj = torch.nn.Linear(d_out, d_out)\n",
        "        self.dropout = torch.nn.Dropout(p = 0.5)\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal = 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "o3CE6hqmUHZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer block of GPT-2\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in = cfg[\"embedding_dim\"],\n",
        "            d_out =  cfg[\"embedding_dim\"],\n",
        "            context_length = cfg[\"context_length\"],\n",
        "            dropout = cfg[\"drop_rate\"],\n",
        "            num_heads = cfg[\"n_heads\"],\n",
        "            qkv_bias = cfg[\"qkv_bias\"]\n",
        "        )\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"embedding_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"embedding_dim\"])\n",
        "        self.drop = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.drop(x)\n",
        "        x += shortcut\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop(x)\n",
        "        x += shortcut\n",
        "        return x"
      ],
      "metadata": {
        "id": "Zs4q8FB7ORD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2, 4, 768)\n",
        "block = TransformerBlock(GPT_2_CONFIG)\n",
        "output = block(x)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhX6YCakZNfu",
        "outputId": "5a324558-a29a-4915-bb2a-f464caaacbba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1890,  1.6600,  0.2652,  ..., -0.8592, -0.4959,  1.1356],\n",
            "         [-0.2148,  1.8773,  2.0456,  ...,  1.5177,  0.1359,  1.9233],\n",
            "         [ 1.4081,  0.1454,  0.3777,  ...,  1.1837,  0.0310,  1.7865],\n",
            "         [-0.1163,  1.0859,  0.4263,  ...,  0.7645,  0.7893,  1.1430]],\n",
            "\n",
            "        [[ 0.0382, -1.0362, -0.0363,  ...,  0.6393, -0.1076,  0.1608],\n",
            "         [ 0.6126,  0.0068,  0.7614,  ...,  1.3073,  0.3980,  1.3227],\n",
            "         [ 0.2942,  0.2697,  1.2901,  ...,  0.9598,  0.6771,  0.4501],\n",
            "         [ 0.1525, -0.5742,  0.2605,  ...,  0.8544,  0.2254,  1.6791]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# final GPT model\n",
        "class GPTmodel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"embedding_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"embedding_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "        self.trf_block = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "        self.final_norm = LayerNorm(cfg[\"embedding_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"embedding_dim\"], cfg[\"vocab_size\"], bias = False)\n",
        "\n",
        "    def forward(self, i_idx):\n",
        "        batch_size, seq_len = i_idx.shape\n",
        "        token_embeds = self.token_emb(i_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = i_idx.device))\n",
        "        x = token_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_block(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "fFW2Y7plZmeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instance\n",
        "torch.manual_seed(123)\n",
        "model = GPTmodel(GPT_2_CONFIG)\n",
        "output = model(batch)\n",
        "print(output)\n",
        "print(\"Input batch:\", batch)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKBF6cELagYl",
        "outputId": "a08d4cdf-b6be-467e-c3c0-e43e41db199f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.5514, -0.9670,  0.7579,  ..., -0.6175, -1.3106, -0.0158],\n",
            "         [ 0.7144, -1.6823, -0.0286,  ..., -0.1798, -0.9726,  0.0926],\n",
            "         [ 0.5534, -1.5468, -0.1502,  ..., -0.1483, -1.8168, -0.6314],\n",
            "         [-0.8051, -1.6082,  0.0258,  ...,  0.2877, -1.0677,  0.3591]],\n",
            "\n",
            "        [[-0.9335, -1.4204,  0.3273,  ..., -0.4144, -1.3657, -0.0283],\n",
            "         [-0.2963, -1.4595, -0.3406,  ..., -0.1631, -1.3662,  0.4405],\n",
            "         [ 0.8794, -0.8463,  0.0735,  ...,  0.3200, -1.7805,  0.0260],\n",
            "         [-0.1378, -1.1232,  0.5363,  ...,  1.0011, -1.5238, -0.1419]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Input batch: tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "Output shape: torch.Size([2, 4, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter size\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total Parameters:\", total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08EQLVc9brfV",
        "outputId": "6282c30d-a550-4ebc-f37c-2ff100f26d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 163009536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# storage size\n",
        "total_size_bytes = total_params * 4\n",
        "total_size = total_size_bytes / (1024 * 1024)\n",
        "print(f\"Total size: {total_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMl3y2bZcTMo",
        "outputId": "3c8f0502-82a3-4f0f-a75b-8ed8a0d477ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size: 621.83 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating text"
      ],
      "metadata": {
        "id": "vG978MyrzL1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function\n",
        "def generate_text(model, idx, max_new_tokens, context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = torch.softmax(logits, dim = -1)\n",
        "        idx_next = torch.argmax(probs, dim = -1, keepdim = True)\n",
        "        idx = torch.cat((idx, idx_next), dim = 1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "ZhE5qTjqzNSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "YOBe0OzkgL9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = \"Hello, I am\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "print(encoded)\n",
        "print(encoded_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx976jsrfuVO",
        "outputId": "718d3012-4d91-491d-daed-014bec283eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 314, 716]\n",
            "torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "out = generate_text(model, encoded_tensor, max_new_tokens = 6, context_size = GPT_2_CONFIG[\"context_length\"])\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvs7X_rMgJ7F",
        "outputId": "6e6c68b6-a13e-4fb2-a6b9-8aa73ce5238b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[15496,    11,   314,   716, 35482]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kljsemthiWY",
        "outputId": "98c30727-94f3-4c2d-d6e4-6baa7e72b27e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am standout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LPPkYPsglLvb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}